{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Sensor\n",
    "\n",
    "## Goal\n",
    "- learn what makes a camera useful for self-driving cars\n",
    "- learn the characteristics of a camera as a sensor, and how images are formed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pinhole Camera Model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Camera Projective Geometry\n",
    "\n",
    "Projection from world coordinates -> image coordinates\n",
    "- project from world coordinates -> camera coordinates\n",
    "- project from camera coordinates -> image coordinates\n",
    "- project from image coordinates -> pixel coordinates by discretization, scaling and offset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "World -> Camera \n",
    "\n",
    "$$\n",
    "O_c = \\begin{bmatrix}\n",
    "            R & t \\\\\n",
    "            0 & 1\n",
    "        \\end{bmatrix} \n",
    "        O_w\n",
    "    = T O_w\n",
    "$$\n",
    "\n",
    "\n",
    "Camera -> image\n",
    "\n",
    "$$\n",
    "O_{i} = \\begin{bmatrix}\n",
    "            f & 0 & u_0 \\\\\n",
    "            0 & f & v_0 \\\\\n",
    "            0 & 0 & 1\n",
    "        \\end{bmatrix} \n",
    "        O_c\n",
    "    = K O_c\n",
    "$$\n",
    "\n",
    "Therefore, world -> image\n",
    "\n",
    "$$\n",
    "O_i = \\begin{bmatrix}\n",
    "        x_i \\\\\n",
    "        y_i \\\\\n",
    "        z_i\n",
    "    \\end{bmatrix}\n",
    "    = K T O_w \n",
    "    = K T \\begin{bmatrix}\n",
    "            x_w \\\\\n",
    "            y_w \\\\\n",
    "            z_w \n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note, $K$ is 3-by-3, $T$ is 4-by-4, $P$ is 3-by-4, and $O_w$ is 3-by-1 by using homogeneous coordinates.\n",
    "$KT$ is not matrix multiplication, but a composition of two transformations, $KT=KR+t$.\n",
    "\n",
    "image -> pixel\n",
    "\n",
    "$$\n",
    "O_i = \\begin{bmatrix}\n",
    "        x_i \\\\\n",
    "        y_i \\\\\n",
    "        z_i\n",
    "    \\end{bmatrix}\n",
    "    \\rightarrow\n",
    "    \\begin{bmatrix}\n",
    "        x_p \\\\\n",
    "        y_p \\\\\n",
    "        1 \\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\frac{1}{z_i}\\begin{bmatrix}\n",
    "        x_i \\\\\n",
    "        y_i \\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Camera Calibration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual Depth Perception\n",
    "\n",
    "- stereo sensor -> how two cameras are related\n",
    "- derive the location of a point in 3D given its projection on the two images of a stereo sensor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Stereo Sensor Model\n",
    "\n",
    "![image.png](./perception/resources/img/stereo-camera-model.png)\n",
    "\n",
    "$O_L$ and $O_R$ are the optical centers of the two cameras, and $f$ is the focal length of the cameras.\n",
    "$b$ is the baseline between the two cameras, and $Z$ is the depth of the point $O$. \n",
    "\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "- sensor is constructed from two identifica cameras\n",
    "- two cameras have parallel optical axes\n",
    "\n",
    "\n",
    "![model](./perception/resources/img/stereo-camera-model-1.png)\n",
    "\n",
    "left camera\n",
    "\n",
    "$$\n",
    "\\frac{Z}{f} = \\frac{X}{x_L}\n",
    "$$\n",
    "\n",
    "right camera\n",
    "\n",
    "$$\n",
    "\\frac{Z}{f} = \\frac{X-b}{x_R}\n",
    "$$ \n",
    "\n",
    "Therefore, we can computer 3D point coordinates from the two images.\n",
    "\n",
    "Lets define disparity $d$ as the difference between the two image coordinates of the same point.\n",
    "\n",
    "$$\n",
    "d = x_L - x_R\n",
    "$$\n",
    "\n",
    "where ($x_L$, $y_L$) and ($x_R$, $y_R$) are the image coordinates of the same point in the left and right images, respectively. The image coordiates are measured in pixels and can be calculated from the pixel coordinates as follows:\n",
    "$$ x_L = u_L - u_0 $$\n",
    "$$ x_R = u_R - u_0 $$\n",
    "$$ y_L = v_L - v_0 $$\n",
    "$$ y_R = v_R - v_0 $$\n",
    "\n",
    "\n",
    "Combing all equations above, we can get the 3D point coordinates for the point (X,Y,Z) in the camera coordinate system.\n",
    "\n",
    "$$ Zx_L = fX $$\n",
    "$$ Zx_R = fX-fb $$\n",
    "$$ Zx_R = Zx_L - fb $$\n",
    "\n",
    "The coordinates of the given point (X,Y,Z) is then obtained as:\n",
    "\n",
    "$$ Z = \\frac{fb}{x_L - x_R} = \\frac{fb}{d} $$\n",
    "$$ X = \\frac{Zx_L}{f} $$\n",
    "$$ Y = \\frac{Zy_L}{f} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main problems:\n",
    "- need to know $f$, $b$, $u_0$, $v_0$\n",
    "    - use stereo camera calibration\n",
    "- need to know the disparity $x_L$, $x_R$ so that $d$ can be calculated\n",
    "    - use disparity computation algorithms based on image matching\n",
    "        - correspond pixels in the left image to those in the right image to find matches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Disparity Computation\n",
    "\n",
    "#### 4.2.1 Epipolar Line\n",
    "\n",
    "![epipolar](./perception/resources/img/epipolar-line.png)\n",
    "\n",
    "Horizontal epipolar line only occur when the optical axes of the two cameras are parallel.\n",
    "\n",
    "If the condition is not met, epipolar lines are not horizontal but instead skewed, and the disparity is not constant along the epipolar line.\n",
    "- In this scenario, we can use stereo retification to warp the images so that the epipolar lines become horizontal.\n",
    "\n",
    "\n",
    "### 4.2.2 Disparity Computation\n",
    "\n",
    "Given rectified images and stereo calibrations:\n",
    "\n",
    "- For each epipolar line,\n",
    "    1. take each pixel on this line in the left image\n",
    "    2. compare these left image pixels to every pixel in the right image on the same epipolar line\n",
    "    3. select the right image pixel that matches the left pixel the most closely, which can be done by minimizng the cost, such as the sum of squared differences (SSD) between the two pixels.\n",
    "    4. compute the disparity as the difference between the column indices of the two pixels\n",
    "\n",
    "\n",
    "Very well-studied region:\n",
    "- survey at http://vision.middlebury.edu/stereo/eval3. \n",
    "- many algorithms have been proposed, such as block matching, semi-global matching, and deep learning based methods.\n",
    "- benchmark tests are also available, such as the Middlebury Stereo Evaluation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Image Filter\n",
    "\n",
    "noise in image -> filter -> denoised image\n",
    "\n",
    "Noise types:\n",
    "- Gaussian noise\n",
    "- Salt-and-pepper noise\n",
    "- Speckle noise\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Cross-Corelation\n",
    " \n",
    "Salt-and-pepper noise usually occurs in low-light conditions, and usually results in outlier pixels with very high value in a low-value neighborhood or very low intensity values in a high-value neighborhood.\n",
    "The following matrix shows an example of the salt-and-pepper noise.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    0 & 0 & 0 & 0 & 0 \\\\\n",
    "    0 & 0 & 0 & 0 & 0 \\\\\n",
    "    0 & 0 & 255 & 0 & 0 \\\\\n",
    "    0 & 0 & 0 & 0 & 0 \\\\\n",
    "    0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To reduce this type of noise, we can use the mean filter, which replaces each pixel with the mean of its neighborhood.\n",
    "\n",
    "$$\n",
    "    G[u,v] = \\frac{1}{(2k+1)^2} \\sum_{i=-k}^{k} \\sum_{j=-k}^{k} I[u+i, v+j]\n",
    "$$\n",
    "\n",
    "where $I$ is the input image, $G$ is the output image, and $k$ is the size of the neighborhood, $2k+1$ is the size of the filter.\n",
    "\n",
    "A more general form of the filter can be represented as:\n",
    "\n",
    "$$\n",
    "    G[u,v] = \\sum_{i=-k}^{k} \\sum_{j=-k}^{k} H[i,j]I[u+i, v+j]\n",
    "$$\n",
    "\n",
    "where $H[i,j]$ is the filter kernel.\n",
    "\n",
    "The mean filter above is a special case of this general form, where $H[i,j] = \\frac{1}{(2k+1)^2}$. If k=1, then $H[i,j] = \\frac{1}{9}$.\n",
    "\n",
    "$$\n",
    "H = \\frac{1}{9} \\begin{bmatrix}\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Similarly, for a Gaussian filter, we can use the following kernel:\n",
    "\n",
    "$$\n",
    "H = \\frac{1}{16} \\begin{bmatrix}\n",
    "    1 & 2 & 1 \\\\\n",
    "    2 & 4 & 2 \\\\\n",
    "    1 & 2 & 1 \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "However, implementing linear filter such as mean filter or Gaussian filter will result in blurring of the image, which is not desirable in many applications.\n",
    "These filters can be tuned to reduced the blurring effect, but the noise reduction will be compromised. There is a tradeoff.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Convolution\n",
    "\n",
    "A covolution is a cross-corrleation where the filter is flipped both horizontally and vertically before being applied to the image.\n",
    "\n",
    "$$\n",
    "    G[u,v] = \\sum_{i=-k}^{k} \\sum_{j=-k}^{k} H[i,j]I[u-i, v-j]\n",
    "$$\n",
    "\n",
    "Unlike cross-correlation, convolution is commutative, which means that the order of the filter and the image does not matter.\n",
    "If $H$ and $F$ are filter kernels, then $H*(F*I) = H*F*I$.\n",
    "Precompute filter convolutions ($H*F$) then apply it to image can reduce time.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Applications\n",
    "- cross correlation\n",
    "    - template matching\n",
    "        - the pixel with the highest response from cross-correlation is the location of the template in the image\n",
    "        - can be used to detect objects in an image, such as lanes\n",
    "    - gradient computation\n",
    "        - define a finite difference kernel\n",
    "        - apply the kernel to the image, and get the image gradient \n",
    "        - very useful for edge detection\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
