{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Sensor\n",
    "\n",
    "## Goal\n",
    "- learn what makes a camera useful for self-driving cars\n",
    "- learn the characteristics of a camera as a sensor, and how images are formed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pinhole Camera Model\n",
    "\n",
    "![pinhole_camera_model](./resources/img/pinhole-camera-geometry.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 From World Coordinate to Camera Coordinate (3D to 3D) \n",
    "\n",
    "![pinhole-transform](./resources/img/pinhole-camera-transform.png)\n",
    "\n",
    "Say the world coordinate is centered at $O(0,0,0)$, and camera coordinate is centered at $C$, where $C$'s coordinates in the world coordinate is $(X_c, Y_c, Z_c)$.\n",
    "\n",
    "The easist transformation between the world coordinate and the camera coordinate is translation, which is performed by a translation vector $t$.\n",
    "In this case, if only translation is performed, a 3D point $X_w$ in the world coordinate can be transformed to a 3D point $X_c$ in the camera coordinate by the following equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_c = \\mathbf{X}_w - t_{OC}\n",
    "$$\n",
    "\n",
    "The tranlsation vector $t_{OC}$ that translate $O$ to $C$ is:\n",
    "\n",
    "$$\n",
    "t_{OC} = C - O = C\n",
    "$$\n",
    "\n",
    "therefore, the following equation holds for translation only:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_c = \\mathbf{X}_w - t_{OC} = \\mathbf{X}_w - C\n",
    "$$\n",
    "\n",
    "\n",
    "Now consider rotation. \n",
    "If besides translation, the camera coordinate is also rotated from the world coordinate, then by simply using the above relationship between $\\mathbf{X}_c$ and $\\mathbf{X}_w$ will not result in the fact that $\\mathbf{X}_c$ and $\\mathbf{X}_w$ should be the same point in the world coordinate.\n",
    "We need apply a rotation matrix $R$ to $\\mathbf{X}_w$ to get $\\mathbf{X}_c$ in this case:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_c = R (\\mathbf{X}_w - C)\n",
    "$$\n",
    "which represent the relationship in inhomegeneous coordinates.\n",
    "\n",
    "Optionally in homegenous coordinates, we can use the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "X_c \\\\\n",
    "Y_c \\\\\n",
    "Z_c \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "R & -RC \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "X_w \\\\\n",
    "Y_w \\\\\n",
    "Z_w \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Camera Coordinate to Image Coordinate (3D to 2D)\n",
    "Within the camera coordinate, then the 3D object is projected to the image plane, which is perpendicular to the optical axis of the camera.\n",
    "\n",
    "The general mapping of a pinhole camera model is then:\n",
    "\n",
    "$$\n",
    "P = KR[I | -C]\n",
    "$$\n",
    "\n",
    "where $P$ is a $3 \\times 4$ projection matrix, $K$ is the $3 \\times 3$ camera intrinsic matrix, $R$ is the $3 \\times 3$ rotation matrix, $I$ is a $3 \\times 3$ identity matrix, and $C$ is the $3 \\times 1$ translation matrix.\n",
    "The above formulation performs translation first then rotation.\n",
    "\n",
    "Another way to write the mapping is:\n",
    "\n",
    "$$\n",
    "P = K[R|t]\n",
    "$$\n",
    "\n",
    "where $t = -RC$\n",
    "This formulation performs rotation first then translation.\n",
    "\n",
    "The camera matrix relates the 3D world coordinate to the 2D image coordinate by:\n",
    "\n",
    "$$ x = P \\mathbf{X}$$\n",
    "where $x$ and $\\mathbf{X}$ are in homogeneous coordinates.\n",
    "\n",
    "\n",
    "The above formulation hints that the projection matrix can typically be decomposed into two matrices: the camera intrinsic matrix and the extrinsic matrix.\n",
    "\n",
    "$$\n",
    "P = K[R|t]\n",
    "$$\n",
    "\n",
    "$$ \n",
    "P = \n",
    "    \\begin{bmatrix}\n",
    "    f & 0 & p_x \\\\\n",
    "    0 & f & p_y \\\\\n",
    "    0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    r_{11} & r_{12} & r_{13} & t_1 \\\\\n",
    "    r_{21} & r_{22} & r_{23} & t_2 \\\\\n",
    "    r_{31} & r_{32} & r_{33} & t_3\n",
    "    \\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "R = \\begin{bmatrix}\n",
    "    r_{11} & r_{12} & r_{13} \\\\\n",
    "    r_{21} & r_{22} & r_{23} \\\\\n",
    "    r_{31} & r_{32} & r_{33}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "t = \\begin{bmatrix}\n",
    "    t_1 \\\\\n",
    "    t_2 \\\\\n",
    "    t_3\n",
    "    \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Camera Projective Geometry\n",
    "\n",
    "Projection from world coordinates -> image coordinates\n",
    "- project from world coordinates -> camera coordinates\n",
    "- project from camera coordinates -> image coordinates\n",
    "- project from image coordinates -> pixel coordinates by discretization, scaling and offset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "World -> Camera \n",
    "\n",
    "$$\n",
    "O_c = \\begin{bmatrix}\n",
    "            R & t \\\\\n",
    "            0 & 1\n",
    "        \\end{bmatrix} \n",
    "        O_w\n",
    "    = T O_w\n",
    "$$\n",
    "\n",
    "\n",
    "Camera -> image\n",
    "\n",
    "$$\n",
    "O_{i} = \\begin{bmatrix}\n",
    "            f & 0 & u_0 \\\\\n",
    "            0 & f & v_0 \\\\\n",
    "            0 & 0 & 1\n",
    "        \\end{bmatrix} \n",
    "        O_c\n",
    "    = K O_c\n",
    "$$\n",
    "\n",
    "Therefore, world -> image\n",
    "\n",
    "$$\n",
    "O_i = \\begin{bmatrix}\n",
    "        x_i \\\\\n",
    "        y_i \\\\\n",
    "        z_i\n",
    "    \\end{bmatrix}\n",
    "    = K T O_w \n",
    "    = K T \\begin{bmatrix}\n",
    "            x_w \\\\\n",
    "            y_w \\\\\n",
    "            z_w \n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note, $K$ is 3-by-3, $T$ is 4-by-4, $P$ is 3-by-4, and $O_w$ is 3-by-1 by using homogeneous coordinates.\n",
    "$KT$ is not matrix multiplication, but a composition of two transformations, $KT=KR+t$.\n",
    "\n",
    "image -> pixel\n",
    "\n",
    "$$\n",
    "O_i = \\begin{bmatrix}\n",
    "        x_i \\\\\n",
    "        y_i \\\\\n",
    "        z_i\n",
    "    \\end{bmatrix}\n",
    "    \\rightarrow\n",
    "    \\begin{bmatrix}\n",
    "        x_p \\\\\n",
    "        y_p \\\\\n",
    "        1 \\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\frac{1}{z_i}\\begin{bmatrix}\n",
    "        x_i \\\\\n",
    "        y_i \\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Camera Calibration\n",
    "\n",
    "Camera matrix $P$ can be estimated from real world data, such as a calibration board with known geometry.\n",
    "\n",
    "The calibration problem is to find the camera intrinsic matrix $K$ and the extrinsic matrix $R$ and $t$ given a set of matched points ${\\mathbf{X}_i, x_i}$, and a camera model $x = P \\mathbf{X}$.\n",
    "$\\mathbf{X}_i$ is the 3D point in the world coordinate and $x_i$ is the corresponding 2D point in the image coordinate.\n",
    "$P$ is pose estmation, and we use a perspective camera model here.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "    z\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "    p_1 & p_2 & p_3 & p_4 \\\\\n",
    "    p_5 & p_6 & p_7 & p_8 \\\\\n",
    "    p_9 & p_{10} & p_{11} & p_{12}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    X \\\\\n",
    "    Y \\\\\n",
    "    Z \\\\\n",
    "    1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "By rearranging the above equation, we can make it homegeneous:\n",
    "\n",
    "$$\n",
    "A x = 0\n",
    "$$\n",
    "\n",
    "where $x$ is the vectorized form of $P$.\n",
    "\n",
    "Therefore, we can use a few methods to solve the linear equations:\n",
    "- SVD -> $A = U \\Epsilon V^T$, and $x$ is the column of $V$ corresponding to the smallest singular value.\n",
    "- Eigenvalue decomposition -> $A^T A v = \\lambda v$, and $x$ is the eigenvector of $A^T A$ corresponding to the smallest eigenvalue.\n",
    "- QR decomposition -> $A = QR$, and $x$ is the last column of $R^{-1} Q^T$.\n",
    "\n",
    "The above methods are all based on the linear algebra, and they are not robust to outliers.\n",
    "Therefore, we can use the RANSAC algorithm to solve the problem.\n",
    "\n",
    "**After we have $P$, how to get $K$, $R$ and $t$?**\n",
    "\n",
    "Decomposition of $P$: \n",
    "$$ \n",
    "P = K[R|t] = K[R | -RC] =[M| -MC]\n",
    "$$\n",
    "\n",
    "Therefore, the problem now is:\n",
    "- find the camera center $C$ -> what is the projection of the camera center?\n",
    "    - $PC = 0$ -> how to compute $C$ from this?\n",
    "        - SVD of $P$. $C$ is the eigenvector corresponding to the smallest eigenvalue\n",
    "- find instrinsic matrix $K$ and rotation matrix $R$ -> how to decompose $M$ because $M=KR$?\n",
    "    - any useful properties of $K$ and $R$ that we can use?\n",
    "        - $K$ is right upper triangular\n",
    "        - $R$ is orthogonal\n",
    "    - QR decomposition of $M$ -> $M = QR$, and $K = Q^T$ and $R = R$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual Depth Perception\n",
    "\n",
    "- stereo sensor -> how two cameras are related\n",
    "- derive the location of a point in 3D given its projection on the two images of a stereo sensor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Stereo Sensor Model\n",
    "\n",
    "![image.png](./resources/img/stereo-camera-model.png)\n",
    "\n",
    "$O_L$ and $O_R$ are the optical centers of the two cameras, and $f$ is the focal length of the cameras.\n",
    "$b$ is the baseline between the two cameras, and $Z$ is the depth of the point $O$. \n",
    "\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "- sensor is constructed from two identifica cameras\n",
    "- two cameras have parallel optical axes\n",
    "\n",
    "\n",
    "![model](./resources/img/stereo-camera-model-1.png)\n",
    "\n",
    "left camera\n",
    "\n",
    "$$\n",
    "\\frac{Z}{f} = \\frac{X}{x_L}\n",
    "$$\n",
    "\n",
    "right camera\n",
    "\n",
    "$$\n",
    "\\frac{Z}{f} = \\frac{X-b}{x_R}\n",
    "$$ \n",
    "\n",
    "Therefore, we can computer 3D point coordinates from the two images.\n",
    "\n",
    "Lets define disparity $d$ as the difference between the two image coordinates of the same point.\n",
    "\n",
    "$$\n",
    "d = x_L - x_R\n",
    "$$\n",
    "\n",
    "where ($x_L$, $y_L$) and ($x_R$, $y_R$) are the image coordinates of the same point in the left and right images, respectively. The image coordiates are measured in pixels and can be calculated from the pixel coordinates as follows:\n",
    "$$ x_L = u_L - u_0 $$\n",
    "$$ x_R = u_R - u_0 $$\n",
    "$$ y_L = v_L - v_0 $$\n",
    "$$ y_R = v_R - v_0 $$\n",
    "\n",
    "\n",
    "Combing all equations above, we can get the 3D point coordinates for the point (X,Y,Z) in the camera coordinate system.\n",
    "\n",
    "$$ Zx_L = fX $$\n",
    "$$ Zx_R = fX-fb $$\n",
    "$$ Zx_R = Zx_L - fb $$\n",
    "\n",
    "The coordinates of the given point (X,Y,Z) is then obtained as:\n",
    "\n",
    "$$ Z = \\frac{fb}{x_L - x_R} = \\frac{fb}{d} $$\n",
    "$$ X = \\frac{Zx_L}{f} $$\n",
    "$$ Y = \\frac{Zy_L}{f} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main problems:\n",
    "- need to know $f$, $b$, $u_0$, $v_0$\n",
    "    - use stereo camera calibration\n",
    "- need to know the disparity $x_L$, $x_R$ so that $d$ can be calculated\n",
    "    - use disparity computation algorithms based on image matching\n",
    "        - correspond pixels in the left image to those in the right image to find matches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Disparity Computation\n",
    "\n",
    "#### 4.2.1 Epipolar Line\n",
    "\n",
    "![epipolar](./resources/img/epipolar-line.png)\n",
    "\n",
    "Horizontal epipolar line only occur when the optical axes of the two cameras are parallel.\n",
    "\n",
    "If the condition is not met, epipolar lines are not horizontal but instead skewed, and the disparity is not constant along the epipolar line.\n",
    "- In this scenario, we can use stereo retification to warp the images so that the epipolar lines become horizontal.\n",
    "\n",
    "\n",
    "### 4.2.2 Disparity Computation\n",
    "\n",
    "Given rectified images and stereo calibrations:\n",
    "\n",
    "- For each epipolar line,\n",
    "    1. take each pixel on this line in the left image\n",
    "    2. compare these left image pixels to every pixel in the right image on the same epipolar line\n",
    "    3. select the right image pixel that matches the left pixel the most closely, which can be done by minimizng the cost, such as the sum of squared differences (SSD) between the two pixels.\n",
    "    4. compute the disparity as the difference between the column indices of the two pixels\n",
    "\n",
    "\n",
    "Very well-studied region:\n",
    "- survey at http://vision.middlebury.edu/stereo/eval3. \n",
    "- many algorithms have been proposed, such as block matching, semi-global matching, and deep learning based methods.\n",
    "- benchmark tests are also available, such as the Middlebury Stereo Evaluation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Image Filter\n",
    "\n",
    "noise in image -> filter -> denoised image\n",
    "\n",
    "Noise types:\n",
    "- Gaussian noise\n",
    "- Salt-and-pepper noise\n",
    "- Speckle noise\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Cross-Corelation\n",
    " \n",
    "Salt-and-pepper noise usually occurs in low-light conditions, and usually results in outlier pixels with very high value in a low-value neighborhood or very low intensity values in a high-value neighborhood.\n",
    "The following matrix shows an example of the salt-and-pepper noise.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    0 & 0 & 0 & 0 & 0 \\\\\n",
    "    0 & 0 & 0 & 0 & 0 \\\\\n",
    "    0 & 0 & 255 & 0 & 0 \\\\\n",
    "    0 & 0 & 0 & 0 & 0 \\\\\n",
    "    0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To reduce this type of noise, we can use the mean filter, which replaces each pixel with the mean of its neighborhood.\n",
    "\n",
    "$$\n",
    "    G[u,v] = \\frac{1}{(2k+1)^2} \\sum_{i=-k}^{k} \\sum_{j=-k}^{k} I[u+i, v+j]\n",
    "$$\n",
    "\n",
    "where $I$ is the input image, $G$ is the output image, and $k$ is the size of the neighborhood, $2k+1$ is the size of the filter.\n",
    "\n",
    "A more general form of the filter can be represented as:\n",
    "\n",
    "$$\n",
    "    G[u,v] = \\sum_{i=-k}^{k} \\sum_{j=-k}^{k} H[i,j]I[u+i, v+j]\n",
    "$$\n",
    "\n",
    "where $H[i,j]$ is the filter kernel.\n",
    "\n",
    "The mean filter above is a special case of this general form, where $H[i,j] = \\frac{1}{(2k+1)^2}$. If k=1, then $H[i,j] = \\frac{1}{9}$.\n",
    "\n",
    "$$\n",
    "H = \\frac{1}{9} \\begin{bmatrix}\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Similarly, for a Gaussian filter, we can use the following kernel:\n",
    "\n",
    "$$\n",
    "H = \\frac{1}{16} \\begin{bmatrix}\n",
    "    1 & 2 & 1 \\\\\n",
    "    2 & 4 & 2 \\\\\n",
    "    1 & 2 & 1 \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "However, implementing linear filter such as mean filter or Gaussian filter will result in blurring of the image, which is not desirable in many applications.\n",
    "These filters can be tuned to reduced the blurring effect, but the noise reduction will be compromised. There is a tradeoff.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Convolution\n",
    "\n",
    "A covolution is a cross-corrleation where the filter is flipped both horizontally and vertically before being applied to the image.\n",
    "\n",
    "$$\n",
    "    G[u,v] = \\sum_{i=-k}^{k} \\sum_{j=-k}^{k} H[i,j]I[u-i, v-j]\n",
    "$$\n",
    "\n",
    "Unlike cross-correlation, convolution is commutative, which means that the order of the filter and the image does not matter.\n",
    "If $H$ and $F$ are filter kernels, then $H*(F*I) = H*F*I$.\n",
    "Precompute filter convolutions ($H*F$) then apply it to image can reduce time.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Applications\n",
    "- cross correlation\n",
    "    - template matching\n",
    "        - the pixel with the highest response from cross-correlation is the location of the template in the image\n",
    "        - can be used to detect objects in an image, such as lanes\n",
    "- convolution\n",
    "    - gradient computation\n",
    "        - define a finite difference kernel\n",
    "        - apply the kernel to the image, and get the image gradient \n",
    "        - very useful for edge detection\n",
    "\n",
    "a 3x3 Horizontal Sobel filter for computing the horizontal gradient:\n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "    1 & 2 & 1 \\\\\n",
    "    0 & 0 & 0 \\\\\n",
    "    -1 & -2 & -1 \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Camera model\n",
    "    - ideal model: pinhole camera model\n",
    "    - real model: nonlinear lens distortion\n",
    "        - correct distortion \n",
    "    - camera calibration\n",
    "        - given a set of 3D points and their corresponding 2D image points, estimate the camera parameters\n",
    "            - method 1: linear equations\n",
    "                - undistort images\n",
    "                - estimate the camera parameters with undistorted image and 3D points using linear equations\n",
    "                - factorize the camera matrix to get the camera parameters\n",
    "            - method 2: nonlinear estimation\n",
    "                - estimate the camera parameters using nonlinear optimization by minimizing the reprojection error\n",
    "\n",
    "- Depth estimation\n",
    "    - stereo camera\n",
    "        - two cameras with known relative pose\n",
    "        - disparity computation - `Block matching algorithms`\n",
    "            - epipolar line\n",
    "            - disparity computation\n",
    "- Image filter\n",
    "    - cross-correlation\n",
    "    - convolution\n",
    "    - applications\n",
    "        - template matching\n",
    "        - gradient computation for edge detection\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
