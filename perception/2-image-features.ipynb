{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Features\n",
    "\n",
    "## Goals:\n",
    "- learn feature extraction, first step of use image features for applications\n",
    "- learn what characteristics make good image features\n",
    "- learn different algorithms used to extract features in images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Dectecion\n",
    "- features are points of interest in an image\n",
    "- points of interest should have the following characteristics:\n",
    "    - Saliency: distinctive, identifiable, and different from its imemediate neighborhood\n",
    "    - Reppeatability: can be found in multiple images using same operations\n",
    "    - Locality: occupies a relatively small subset of image space\n",
    "        - should not change much under small changes in image\n",
    "    - Quantity: enough points represented in the image\n",
    "    - Efficency: reasonable computing time as a preprocessing step\n",
    "\n",
    "\n",
    "Extraction:\n",
    "\n",
    "- repetitive texture less patches are challenging to detect consistently\n",
    "- patches with large contrast changes (gradients) are easier to detect (edges)\n",
    "- gradients in at least two (significantly) different orientations are the easiest to detect (corners)\n",
    "    - famous corner algorithm: Harris Corner Detection\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Feature Detection Algorithms\n",
    "- Harris -> corners\n",
    "  - easy to compute, but not sacale invariant\n",
    "- Harris-Laplace -> corners\n",
    "  - scale invariant\n",
    "- Features from acclerated segment test (FAST) -> corners\n",
    "  - machine learning approach for fast corner detection\n",
    "- Laplacian of Gaussian (LOG) detector -> blobs\n",
    "  - scale invariant, but not rotation invariant\n",
    "- Difference of Gaussian (DOG) detector -> blobs\n",
    "  - approximate LOG, but faster\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Harris Corner Detection\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Descriptors\n",
    "\n",
    "**Goals:**\n",
    "- what characteristics make a good feature descriptor\n",
    "- different algorithms used to extract feature descriptors from images\n",
    "\n",
    "\n",
    "**Features/Descriptors**\n",
    "- Feature: points of interest in an image defined by pixel coordinates $[u,v]$\n",
    "- Descriptor: an N-dimensional vector that provides a summary of the image information around the detected feature $\\{f_1, f_2, ..., f_N\\}$\n",
    "\n",
    "- Feature descriptors should have the following characteristics:\n",
    "    - Distinctiveness: different features should have different descriptors\n",
    "    - Robustness: descriptors should be able to be matched even if the image is transformed\n",
    "    - Compactness: descriptors should be small enough to be stored and compared efficiently\n",
    "    - Efficiency: descriptors should be able to be computed quickly\n",
    "\n",
    "\n",
    "**How to describe a feature**\n",
    "- obsolute intensity values of pixels in a local neighborhood: \n",
    "    - not robust to illumination changes\n",
    "- relative intensity (i.e., gradient) values of pixels in a local neighborhood:\n",
    "    - feature is invariant to obsolute intensity values\n",
    "    - feature is senstive to deformations\n",
    "- color histogram of pixels in a local neighborhood:\n",
    "    - invariant to changes in scale and rotation\n",
    "    - sensitive to spatial layout\n",
    "- spatial histograms\n",
    "    - sensitive to rotation\n",
    "- orientation normalization\n",
    "    - use the dominat image gradient direction to normalize the orientation of the path\n",
    "        - save the orientation angle along with pixel intensity\n",
    "\n",
    "**Multi-scale Oriented Patches (MOPS)**\n",
    "\n",
    "**Histogram of Textons Descriptor**\n",
    "\n",
    "**Histogram of oritend gradients (HOG)**\n",
    "\n",
    "**SIFT: Scale Invariant Feature Transform**\n",
    "- SIFT describes both a detector and descriptor\n",
    "    - detection\n",
    "        - multi-scale extrema detection: Gaussian pyramid + DOG\n",
    "        - keypoint localization\n",
    "        - orientation assignment\n",
    "        - returns: $(x,y) \\rightarrow\\ location, $\\sigma$ $\\rightarrow$ scale, $\\theta$ $\\rightarrow$ orientation\n",
    "\n",
    "    - keypoint descriptor\n",
    "\n",
    "\n",
    "- SIFT is used in many state-of-art systems\n",
    "- Combined with DOG feature detector, SIFT descriptors provide a scalue, rotation, and illumation invariant feature detector/descriptor pair\n",
    "\n",
    "\n",
    "**Other Feature Descriptors**:\n",
    "- Speeded Up Robust Features (SURF)\n",
    "- Gradient Location and Orientation Histogram (GLOH)\n",
    "- Binary Robust Independent Elementary Features (BRIEF)\n",
    "- Oriented FAST and Rotated BRIEF (ORB)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Matching\n",
    "\n",
    "given a feature and its descriptor in image 1, find the best matching in image 2.\n",
    "\n",
    "\n",
    "**Distance Function**\n",
    "\n",
    "- SSD: square sum of differences\n",
    "- SAD: sum of absolute differences\n",
    "- Hamming distance: for binary descriptors\n",
    "\n",
    "**Brute Force Matching**\n",
    "- define a distance function $d(f_i, f_j)$ that compares two descriptors\n",
    "- for every feature $f_i$ in image 1:\n",
    "    - compute distance $d(f_i, f_j)$ for every feature $f_j$ in image 2\n",
    "    - find the `cloest` match $f_c$ that has the smallest distance to $f_i$\n",
    "\n",
    "Problem of the above approach is:\n",
    "- if $f_i$ in image 1 doesn't have a match in image 2, then the `closest` match will be returned as the $f_j$ with the smallest distance, even the distance is pretty big.\n",
    "- solution is to use a threshold to filter out the bad matches\n",
    "\n",
    "**Nearest Neighbor Matching**\n",
    "- define a distance function $d(f_i, f_j)$ that compares two descriptors\n",
    "- define a distance threshold $\\delta$\n",
    "- for every feature $f_i$ in image 1:\n",
    "    - compute distance $d(f_i, f_j)$ for every feature $f_j$ in image 2\n",
    "    - find the `closest` match $f_c$ that has the smallest distance to $f_i$\n",
    "    - if $d(f_i, f_c) < \\delta$, then $f_c$ is a good match for $f_i$, and keep it\n",
    "\n",
    "\n",
    "- Brute force matching might be fast enough for large amounts of features\n",
    "- Use KD-tree or other data structures to speed up nearest neighbor search\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguous Matches\n",
    "\n",
    "- if there are multiple matches for a feature, then the feature is ambiguous\n",
    "- ambiguous matches can be resolved by:\n",
    "    - using a better feature detector/descriptor\n",
    "    - using a better distance function\n",
    "    - using a better matching algorithm\n",
    "    - using a better threshold\n",
    "    - using a better data structure for nearest neighbor search\n",
    "\n",
    "**Distance Ratio**\n",
    "- compute the distance $d(f_i,f_j)$ for each feature $f_i$ in image 1, and featuer $f_j$ in image 2.\n",
    "- find the cloest match $f_c$\n",
    "- find the second closest match $f_s$\n",
    "- find how better the closest match is than the second closest match: $r = \\frac{d(f_i,f_c)}{d(f_i,f_s)}$\n",
    "\n",
    "\n",
    "**Brute Force Matching with Distance Ratio**\n",
    "- define a distance function $d(f_i, f_j)$ that compares two descriptors\n",
    "- define a distance ratio threshold $\\rho$\n",
    "- for every feature $f_i$ in image 1:\n",
    "    - compute distance $d(f_i, f_j)$ for every feature $f_j$ in image 2\n",
    "    - find the `closest` match $f_c$ and the second cloest match $f_s$\n",
    "    - compute their distance ratio $r$\n",
    "    - keep matches with $r < \\rho$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outlier Rejection\n",
    "\n",
    "- outliers are wrong feature matching outputs, that can occur due to errors in any of the three stages of feature detection, description, and matching\n",
    "- RANSAC is a popular algorithm for outlier rejection\n",
    "\n",
    "\n",
    "**Why removing outliers**\n",
    "\n",
    "The purpose of doing feature matching is to find the same feature in two different images, so that we can estimate the transformation between the two images.\n",
    "The transformation of the images can then be used to estimate the camera pose. If there are outliers in the feature matching, then the estimated transformation will be wrong, and the estimated camera pose will be wrong too.\n",
    "\n",
    "**RANSAC**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visual Odemetry\n",
    "\n",
    "**Visual Odemetry (VO)**\n",
    "- the process of estimating the motion of a camera by analyzing the associated image sequences\n",
    "\n",
    "pros:\n",
    "- not affected by wheel slip in uneven terrain, rainy/snowy weather, or other adverse condition\n",
    "- more accurate trajectory estimates compared to wheel odemetry\n",
    "\n",
    "cons:\n",
    "- need an external sensor to estimate absolute scale. scale cannot be estimated by a single sensor\n",
    "- camera is a passive sensor, might not be robust to weather/illumination changes\n",
    "- any form of ordemetry (incremetal state estimation) drifts over time\n",
    "\n",
    "\n",
    "**VO Pipeline**\n",
    "- feature detection\n",
    "- feature matching\n",
    "- outlier rejection\n",
    "- motion estimation\n",
    "- scale estimation\n",
    "- loop closure\n",
    "- map optimization\n",
    "\n",
    "**VO vs SLAM**\n",
    "- VO: only estimate the camera motion\n",
    "- SLAM: estimate the camera motion and the map of the environment\n",
    "\n",
    "**VO Problem**\n",
    "\n",
    "given two frames of an image sequence $I_{k-1}, I_k$, estimate the camera motion $T_k$ between the two frames.\n",
    "\n",
    "$$\n",
    "T_k = \\begin{bmatrix} R_{k,k-1} & t_{k,k-1} \\\\ 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $R_k$ is the rotation matrix, and $t_k$ is the translation vector.\n",
    "\n",
    "Concatenating these single movements allows the recovery of the full trajectory of the cammera\n",
    "\n",
    "\n",
    "Pipeline for solving:\n",
    "    - Given: $I_{k-1}, I_k$\n",
    "    - Extract and match features $f_{k-1}$ and $f_k$\n",
    "    - Estimate motion $T_k$ from $f_{k-1}$ and $f_k$\n",
    "\n",
    "**Motion Estimation**\n",
    "depends on feature representation:\n",
    "- 2D to 2D: both $f_{k-1}$ and $f_k$ are defiend in image coordinates\n",
    "    - for tracking object in image frame\n",
    "    - visual tracking and image stabalization\n",
    "- 3D to 3D: both $f_{k-1}$ and $f_k$ are defiend in 3D \n",
    "    - stereo camera or depth information\n",
    "    - locate image features in 3-D space\n",
    "- 3D to 2D: $f_{k-1}$ is defiend in 3D, and $f_k$ is their corresponding projection defiend in image coordinates\n",
    "\n",
    "**3D-2D Motion Estimation**\n",
    "\n",
    "problem:\n",
    "- 3D world coordinates of features in frame k-1\n",
    "- 2D image coordinates of features in frame k\n",
    "\n",
    "camera projection model:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} su \\\\ sv \\\\ s \\end{bmatrix} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} R & t \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "camera instrinsic matrix $K$ is known from camera calibration, world coordinates and image coordinates are also known.\n",
    "\n",
    "unknowns:\n",
    "- rotation matrix $R$ \n",
    "- translation vector $t$\n",
    "\n",
    "![motion-estimation](./resources/img/motion-estimation.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PNP Perspective-n-Point\n",
    "\n",
    "PnP:\n",
    "- solve for initial guess of $R$ and $t$ using Direct Linear Transform (DLT)\n",
    "    - form a linear model and solve for $R$ and $t$ using SVD\n",
    "- improve solution using Levemberg-Marquardt (LM) optimization\n",
    "- need at least 3 points to solve P3P, 4 if we dont want ambiguous solutions. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
